{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9T0nlIRyEIH"
      },
      "source": [
        "# **Carregar conjunto de dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6no0JjHyA_H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "df = pd.read_excel('/Cap - Rotulados ELECTRE TRI-rC.xlsx')\n",
        "df.set_index('Suppliers', inplace=True)\n",
        "display(df.head())  # Mostra as primeiras linhas do DataFrame\n",
        "\n",
        "\n",
        "\n",
        "X_criterios = df.iloc[:, 0:9].values\n",
        "\n",
        "X_criterios\n",
        "\n",
        "Y_classe = df.iloc[:, 9].values\n",
        "\n",
        "Y_classe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLqWQiuoyMaW"
      },
      "source": [
        "# **Separar conjuntos de treinamento e teste**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf0QguP0yWCA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(X_criterios, Y_classe, test_size = 0.20, random_state = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zakIsuscAo_S"
      },
      "outputs": [],
      "source": [
        "Y_classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuBWVh5hAq_M"
      },
      "outputs": [],
      "source": [
        "Y_classe_teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VJOAXtCEpDM"
      },
      "source": [
        "# **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V1sE0ntcnc4"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Definindo a grade de parâmetros para o GridSearchCV\n",
        "parametros = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "knn_macro_f1_scores = []\n",
        "knn_micro_f1_scores = []\n",
        "knn_balanced_accuracies = []\n",
        "knn_mcc_scores = []\n",
        "knn_best_params_list = []\n",
        "knn_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(\n",
        "        X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Configurando o StratifiedKFold\n",
        "    stratified_k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=KNeighborsClassifier(),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=stratified_k_fold,\n",
        "                               n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    knn_melhores_parametros = grid_search.best_params_\n",
        "    knn_melhor_resultado = grid_search.best_score_\n",
        "    knn_best_params_list.append(knn_melhores_parametros)\n",
        "    knn_best_scores_list.append(knn_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_knn = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_knn.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    knn_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    knn_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    knn_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    knn_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    knn_macro_f1_scores.append(knn_macro_f1)\n",
        "    knn_micro_f1_scores.append(knn_micro_f1)\n",
        "    knn_balanced_accuracies.append(knn_balanced_acc)\n",
        "    knn_mcc_scores.append(knn_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "knn_mean_macro_f1 = np.mean(knn_macro_f1_scores)\n",
        "knn_std_macro_f1 = np.std(knn_macro_f1_scores)\n",
        "knn_mean_micro_f1 = np.mean(knn_micro_f1_scores)\n",
        "knn_std_micro_f1 = np.std(knn_micro_f1_scores)\n",
        "knn_mean_balanced_acc = np.mean(knn_balanced_accuracies)\n",
        "knn_std_balanced_acc = np.std(knn_balanced_accuracies)\n",
        "knn_mean_mcc = np.mean(knn_mcc_scores)\n",
        "knn_std_mcc = np.std(knn_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {knn_mean_macro_f1:.4f} (Desvio Padrão: {knn_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {knn_mean_micro_f1:.4f} (Desvio Padrão: {knn_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {knn_mean_balanced_acc:.4f} (Desvio Padrão: {knn_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {knn_mean_mcc:.4f} (Desvio Padrão: {knn_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "knn_best_overall_params = knn_best_params_list[np.argmax(knn_best_scores_list)]\n",
        "knn_best_overall_score = max(knn_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {knn_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {knn_best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPOoPGCId0tS"
      },
      "outputs": [],
      "source": [
        "knn_macro_f1_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ZuAvfS7OP5"
      },
      "source": [
        "## **Salvando resultados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5kTMhtb4KXv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPSa-rJ94pag"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/knn_results.pkl'\n",
        "\n",
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'knn_macro_f1_scores': knn_macro_f1_scores,\n",
        "        'knn_micro_f1_scores': knn_micro_f1_scores,\n",
        "        'knn_balanced_accuracies': knn_balanced_accuracies,\n",
        "        'knn_mcc_scores': knn_mcc_scores,\n",
        "        'knn_best_params_list': knn_best_params_list,\n",
        "        'knn_best_scores_list': knn_best_scores_list,\n",
        "        'knn_mean_macro_f1': knn_mean_macro_f1,\n",
        "        'knn_std_macro_f1': knn_std_macro_f1,\n",
        "        'knn_mean_micro_f1': knn_mean_micro_f1,\n",
        "        'knn_std_micro_f1': knn_std_micro_f1,\n",
        "        'knn_mean_balanced_acc': knn_mean_balanced_acc,\n",
        "        'knn_std_balanced_acc': knn_std_balanced_acc,\n",
        "        'knn_mean_mcc': knn_mean_mcc,\n",
        "        'knn_std_mcc': knn_std_mcc,\n",
        "        'knn_best_overall_params': knn_best_overall_params,\n",
        "        'knn_best_overall_score': knn_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQKmr-AA7Th5"
      },
      "source": [
        "## **Carregando resultados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFqzYYt54rXX"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/knn_results.pkl'\n",
        "\n",
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        knn_macro_f1_scores = data['knn_macro_f1_scores']\n",
        "        knn_micro_f1_scores = data['knn_micro_f1_scores']\n",
        "        knn_balanced_accuracies = data['knn_balanced_accuracies']\n",
        "        knn_mcc_scores = data['knn_mcc_scores']\n",
        "        knn_best_params_list = data['knn_best_params_list']\n",
        "        knn_best_scores_list = data['knn_best_scores_list']\n",
        "        knn_mean_macro_f1 = data['knn_mean_macro_f1']\n",
        "        knn_std_macro_f1 = data['knn_std_macro_f1']\n",
        "        knn_mean_micro_f1 = data['knn_mean_micro_f1']\n",
        "        knn_std_micro_f1 = data['knn_std_micro_f1']\n",
        "        knn_mean_balanced_acc = data['knn_mean_balanced_acc']\n",
        "        knn_std_balanced_acc = data['knn_std_balanced_acc']\n",
        "        knn_mean_mcc = data['knn_mean_mcc']\n",
        "        knn_std_mcc = data['knn_std_mcc']\n",
        "        knn_best_overall_params = data['knn_best_overall_params']\n",
        "        knn_best_overall_score = data['knn_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    knn_macro_f1_scores = []\n",
        "    knn_micro_f1_scores = []\n",
        "    knn_balanced_accuracies = []\n",
        "    knn_mcc_scores = []\n",
        "    knn_best_params_list = []\n",
        "    knn_best_scores_list = []\n",
        "    knn_mean_macro_f1 = 0\n",
        "    knn_std_macro_f1 = 0\n",
        "    knn_mean_micro_f1 = 0\n",
        "    knn_std_micro_f1 = 0\n",
        "    knn_mean_balanced_acc = 0\n",
        "    knn_std_balanced_acc = 0\n",
        "    knn_mean_mcc = 0\n",
        "    knn_std_mcc = 0\n",
        "    knn_best_overall_params = None\n",
        "    knn_best_overall_score = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9Ld79G2LkKe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4mXaniCLmzl"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Definindo a grade de parâmetros para o GridSearchCV\n",
        "parametros = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "knn_macro_f1_scores = []\n",
        "knn_micro_f1_scores = []\n",
        "knn_balanced_accuracies = []\n",
        "knn_mcc_scores = []\n",
        "knn_best_params_list = []\n",
        "knn_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=KNeighborsClassifier(),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=10)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    knn_melhores_parametros = grid_search.best_params_\n",
        "    knn_melhor_resultado = grid_search.best_score_\n",
        "    knn_best_params_list.append(knn_melhores_parametros)\n",
        "    knn_best_scores_list.append(knn_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_knn = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_knn.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    knn_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    knn_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    knn_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    knn_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    knn_macro_f1_scores.append(knn_macro_f1)\n",
        "    knn_micro_f1_scores.append(knn_micro_f1)\n",
        "    knn_balanced_accuracies.append(knn_balanced_acc)\n",
        "    knn_mcc_scores.append(knn_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "knn_mean_macro_f1 = np.mean(knn_macro_f1_scores)\n",
        "knn_std_macro_f1 = np.std(knn_macro_f1_scores)\n",
        "knn_mean_micro_f1 = np.mean(knn_micro_f1_scores)\n",
        "knn_std_micro_f1 = np.std(knn_micro_f1_scores)\n",
        "knn_mean_balanced_acc = np.mean(knn_balanced_accuracies)\n",
        "knn_std_balanced_acc = np.std(knn_balanced_accuracies)\n",
        "knn_mean_mcc = np.mean(knn_mcc_scores)\n",
        "knn_std_mcc = np.std(knn_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {knn_mean_macro_f1:.4f} (Desvio Padrão: {knn_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {knn_mean_micro_f1:.4f} (Desvio Padrão: {knn_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {knn_mean_balanced_acc:.4f} (Desvio Padrão: {knn_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {knn_mean_mcc:.4f} (Desvio Padrão: {knn_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "knn_best_overall_params = knn_best_params_list[np.argmax(knn_best_scores_list)]\n",
        "knn_best_overall_score = max(knn_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {knn_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {knn_best_overall_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9dEjz-IL0MT"
      },
      "source": [
        "## **Salvando resultados KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7_PRj-TMQ6I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmewBcQ4L2RF"
      },
      "outputs": [],
      "source": [
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JhuyHjrL29B"
      },
      "outputs": [],
      "source": [
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open('knn_results.pkl', 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'knn_macro_f1_scores': knn_macro_f1_scores,\n",
        "        'knn_micro_f1_scores': knn_micro_f1_scores,\n",
        "        'knn_balanced_accuracies': knn_balanced_accuracies,\n",
        "        'knn_mcc_scores': knn_mcc_scores,\n",
        "        'knn_best_params_list': knn_best_params_list,\n",
        "        'knn_best_scores_list': knn_best_scores_list,\n",
        "        'knn_mean_macro_f1': knn_mean_macro_f1,\n",
        "        'knn_std_macro_f1': knn_std_macro_f1,\n",
        "        'knn_mean_micro_f1': knn_mean_micro_f1,\n",
        "        'knn_std_micro_f1': knn_std_micro_f1,\n",
        "        'knn_mean_balanced_acc': knn_mean_balanced_acc,\n",
        "        'knn_std_balanced_acc': knn_std_balanced_acc,\n",
        "        'knn_mean_mcc': knn_mean_mcc,\n",
        "        'knn_std_mcc': knn_std_mcc,\n",
        "        'knn_best_overall_params': knn_best_overall_params,\n",
        "        'knn_best_overall_score': knn_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzawKNGFL3h7"
      },
      "source": [
        "## **Carregando resultados KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--UGzUVWL2w3"
      },
      "outputs": [],
      "source": [
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open('knn_results.pkl', 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        knn_macro_f1_scores = data['knn_macro_f1_scores']\n",
        "        knn_micro_f1_scores = data['knn_micro_f1_scores']\n",
        "        knn_balanced_accuracies = data['knn_balanced_accuracies']\n",
        "        knn_mcc_scores = data['knn_mcc_scores']\n",
        "        knn_best_params_list = data['knn_best_params_list']\n",
        "        knn_best_scores_list = data['knn_best_scores_list']\n",
        "        knn_mean_macro_f1 = data['knn_mean_macro_f1']\n",
        "        knn_std_macro_f1 = data['knn_std_macro_f1']\n",
        "        knn_mean_micro_f1 = data['knn_mean_micro_f1']\n",
        "        knn_std_micro_f1 = data['knn_std_micro_f1']\n",
        "        knn_mean_balanced_acc = data['knn_mean_balanced_acc']\n",
        "        knn_std_balanced_acc = data['knn_std_balanced_acc']\n",
        "        knn_mean_mcc = data['knn_mean_mcc']\n",
        "        knn_std_mcc = data['knn_std_mcc']\n",
        "        knn_best_overall_params = data['knn_best_overall_params']\n",
        "        knn_best_overall_score = data['knn_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    knn_macro_f1_scores = []\n",
        "    knn_micro_f1_scores = []\n",
        "    knn_balanced_accuracies = []\n",
        "    knn_mcc_scores = []\n",
        "    knn_best_params_list = []\n",
        "    knn_best_scores_list = []\n",
        "    knn_mean_macro_f1 = 0\n",
        "    knn_std_macro_f1 = 0\n",
        "    knn_mean_micro_f1 = 0\n",
        "    knn_std_micro_f1 = 0\n",
        "    knn_mean_balanced_acc = 0\n",
        "    knn_std_balanced_acc = 0\n",
        "    knn_mean_mcc = 0\n",
        "    knn_std_mcc = 0\n",
        "    knn_best_overall_params = None\n",
        "    knn_best_overall_score = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a0dWlK5-5Zw"
      },
      "source": [
        "# **Naive Bayes **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itRFnML_7l2K"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Como o Naive Bayes não tem muitos hiperparâmetros para ajustar, podemos usar uma configuração padrão\n",
        "parametros = {\n",
        "    'var_smoothing': np.logspace(0, -9, num=100)\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "nb_macro_f1_scores = []\n",
        "nb_micro_f1_scores = []\n",
        "nb_balanced_accuracies = []\n",
        "nb_mcc_scores = []\n",
        "nb_best_params_list = []\n",
        "nb_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(\n",
        "        X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Configurando o StratifiedKFold\n",
        "    stratified_k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=GaussianNB(),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=stratified_k_fold,\n",
        "                               n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    nb_melhores_parametros = grid_search.best_params_\n",
        "    nb_melhor_resultado = grid_search.best_score_\n",
        "    nb_best_params_list.append(nb_melhores_parametros)\n",
        "    nb_best_scores_list.append(nb_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_nb = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_nb.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    nb_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    nb_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    nb_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    nb_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    nb_macro_f1_scores.append(nb_macro_f1)\n",
        "    nb_micro_f1_scores.append(nb_micro_f1)\n",
        "    nb_balanced_accuracies.append(nb_balanced_acc)\n",
        "    nb_mcc_scores.append(nb_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "nb_mean_macro_f1 = np.mean(nb_macro_f1_scores)\n",
        "nb_std_macro_f1 = np.std(nb_macro_f1_scores)\n",
        "nb_mean_micro_f1 = np.mean(nb_micro_f1_scores)\n",
        "nb_std_micro_f1 = np.std(nb_micro_f1_scores)\n",
        "nb_mean_balanced_acc = np.mean(nb_balanced_accuracies)\n",
        "nb_std_balanced_acc = np.std(nb_balanced_accuracies)\n",
        "nb_mean_mcc = np.mean(nb_mcc_scores)\n",
        "nb_std_mcc = np.std(nb_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {nb_mean_macro_f1:.4f} (Desvio Padrão: {nb_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {nb_mean_micro_f1:.4f} (Desvio Padrão: {nb_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {nb_mean_balanced_acc:.4f} (Desvio Padrão: {nb_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {nb_mean_mcc:.4f} (Desvio Padrão: {nb_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "nb_best_overall_params = nb_best_params_list[np.argmax(nb_best_scores_list)]\n",
        "nb_best_overall_score = max(nb_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {nb_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {nb_best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0BUhW2J8cdT"
      },
      "source": [
        "## **Salvando resultados NB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzWRbRfa8TCT"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBMPNIBB8WhE"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/nb_results.pkl'\n",
        "\n",
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'nb_macro_f1_scores': nb_macro_f1_scores,\n",
        "        'nb_micro_f1_scores': nb_micro_f1_scores,\n",
        "        'nb_balanced_accuracies': nb_balanced_accuracies,\n",
        "        'nb_mcc_scores': nb_mcc_scores,\n",
        "        'nb_best_params_list': nb_best_params_list,\n",
        "        'nb_best_scores_list': nb_best_scores_list,\n",
        "        'nb_mean_macro_f1': nb_mean_macro_f1,\n",
        "        'nb_std_macro_f1': nb_std_macro_f1,\n",
        "        'nb_mean_micro_f1': nb_mean_micro_f1,\n",
        "        'nb_std_micro_f1': nb_std_micro_f1,\n",
        "        'nb_mean_balanced_acc': nb_mean_balanced_acc,\n",
        "        'nb_std_balanced_acc': nb_std_balanced_acc,\n",
        "        'nb_mean_mcc': nb_mean_mcc,\n",
        "        'nb_std_mcc': nb_std_mcc,\n",
        "        'nb_best_overall_params': nb_best_overall_params,\n",
        "        'nb_best_overall_score': nb_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17ufHmpS8hAn"
      },
      "source": [
        "## **Carregando resultados NB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7VIHMju8Z5_"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/nb_results.pkl'\n",
        "\n",
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        nb_macro_f1_scores = data['nb_macro_f1_scores']\n",
        "        nb_micro_f1_scores = data['nb_micro_f1_scores']\n",
        "        nb_balanced_accuracies = data['nb_balanced_accuracies']\n",
        "        nb_mcc_scores = data['nb_mcc_scores']\n",
        "        nb_best_params_list = data['nb_best_params_list']\n",
        "        nb_best_scores_list = data['nb_best_scores_list']\n",
        "        nb_mean_macro_f1 = data['nb_mean_macro_f1']\n",
        "        nb_std_macro_f1 = data['nb_std_macro_f1']\n",
        "        nb_mean_micro_f1 = data['nb_mean_micro_f1']\n",
        "        nb_std_micro_f1 = data['nb_std_micro_f1']\n",
        "        nb_mean_balanced_acc = data['nb_mean_balanced_acc']\n",
        "        nb_std_balanced_acc = data['nb_std_balanced_acc']\n",
        "        nb_mean_mcc = data['nb_mean_mcc']\n",
        "        nb_std_mcc = data['nb_std_mcc']\n",
        "        nb_best_overall_params = data['nb_best_overall_params']\n",
        "        nb_best_overall_score = data['nb_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    nb_macro_f1_scores = []\n",
        "    nb_micro_f1_scores = []\n",
        "    nb_balanced_accuracies = []\n",
        "    nb_mcc_scores = []\n",
        "    nb_best_params_list = []\n",
        "    nb_best_scores_list = []\n",
        "    nb_mean_macro_f1 = 0\n",
        "    nb_std_macro_f1 = 0\n",
        "    nb_mean_micro_f1 = 0\n",
        "    nb_std_micro_f1 = 0\n",
        "    nb_mean_balanced_acc = 0\n",
        "    nb_std_balanced_acc = 0\n",
        "    nb_mean_mcc = 0\n",
        "    nb_std_mcc = 0\n",
        "    nb_best_overall_params = None\n",
        "    nb_best_overall_score = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSThAjmO-meW"
      },
      "outputs": [],
      "source": [
        "nb_macro_f1_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Swry7GyBWug"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Como o Naive Bayes não tem muitos hiperparâmetros para ajustar, podemos usar uma configuração padrão\n",
        "parametros = {\n",
        "    'var_smoothing': np.logspace(0, -9, num=100)\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "macro_f1_scores = []\n",
        "micro_f1_scores = []\n",
        "balanced_accuracies = []\n",
        "mcc_scores = []\n",
        "best_params_list = []\n",
        "best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=GaussianNB(),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=10)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    melhores_parametros = grid_search.best_params_\n",
        "    melhor_resultado = grid_search.best_score_\n",
        "    best_params_list.append(melhores_parametros)\n",
        "    best_scores_list.append(melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_nb = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_nb.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    macro_f1_scores.append(macro_f1)\n",
        "    micro_f1_scores.append(micro_f1)\n",
        "    balanced_accuracies.append(balanced_acc)\n",
        "    mcc_scores.append(mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "mean_macro_f1 = np.mean(macro_f1_scores)\n",
        "std_macro_f1 = np.std(macro_f1_scores)\n",
        "mean_micro_f1 = np.mean(micro_f1_scores)\n",
        "std_micro_f1 = np.std(micro_f1_scores)\n",
        "mean_balanced_acc = np.mean(balanced_accuracies)\n",
        "std_balanced_acc = np.std(balanced_accuracies)\n",
        "mean_mcc = np.mean(mcc_scores)\n",
        "std_mcc = np.std(mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {mean_macro_f1:.4f} (Desvio Padrão: {std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {mean_micro_f1:.4f} (Desvio Padrão: {std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {mean_balanced_acc:.4f} (Desvio Padrão: {std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {mean_mcc:.4f} (Desvio Padrão: {std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "best_overall_params = best_params_list[np.argmax(best_scores_list)]\n",
        "best_overall_score = max(best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {best_overall_params}\")\n",
        "print(f\"Melhor score geral: {best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhqrkQ7gDfFZ"
      },
      "outputs": [],
      "source": [
        "best_scores_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pi6HQfGCo1q"
      },
      "outputs": [],
      "source": [
        "macro_f1_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYo02-RBIBtm"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import f1_score, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "macro_f1_scores = []\n",
        "micro_f1_scores = []\n",
        "balanced_accuracies = []\n",
        "mcc_scores = []\n",
        "\n",
        "for i in range(100):\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Treinando o modelo Naive Bayes\n",
        "    naive_classe = GaussianNB()\n",
        "    naive_classe.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = naive_classe.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    macro_f1_scores.append(macro_f1)\n",
        "    micro_f1_scores.append(micro_f1)\n",
        "    balanced_accuracies.append(balanced_acc)\n",
        "    mcc_scores.append(mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão\n",
        "mean_macro_f1 = np.mean(macro_f1_scores)\n",
        "std_macro_f1 = np.std(macro_f1_scores)\n",
        "mean_micro_f1 = np.mean(micro_f1_scores)\n",
        "std_micro_f1 = np.std(micro_f1_scores)\n",
        "mean_balanced_acc = np.mean(balanced_accuracies)\n",
        "std_balanced_acc = np.std(balanced_accuracies)\n",
        "mean_mcc = np.mean(mcc_scores)\n",
        "std_mcc = np.std(mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {mean_macro_f1:.4f} (Desvio Padrão: {std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {mean_micro_f1:.4f} (Desvio Padrão: {std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {mean_balanced_acc:.4f} (Desvio Padrão: {std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {mean_mcc:.4f} (Desvio Padrão: {std_mcc:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SDmnIUPDYGZ"
      },
      "outputs": [],
      "source": [
        "macro_f1_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wZYaMt8-utI"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(X_criterios, Y_classe, test_size = 0.20, random_state = i)\n",
        "\n",
        "# Treinando o modelo Naive Bayes\n",
        "naive_classe = GaussianNB()\n",
        "naive_classe.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvT3UG_LoodB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-55_4GCnCCn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, balanced_accuracy_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "# Fazendo previsões nos dados de teste\n",
        "Y_previsoes = naive_classe.predict(X_criterios_teste)\n",
        "\n",
        "# Calculando as métricas\n",
        "macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "# Imprimindo as métricas\n",
        "print(\"Macro F1 Score:\", macro_f1)\n",
        "print(\"Micro F1 Score:\", micro_f1)\n",
        "print(\"Balanced Accuracy:\", balanced_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KciET9OYRbe2"
      },
      "outputs": [],
      "source": [
        "Y_classe_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gROpd5ftn4_Y"
      },
      "outputs": [],
      "source": [
        "# Conferindo resultados do conjunto de treinamento também\n",
        "\n",
        "Y_previsoes2 = naive_classe.predict(X_criterios_treinamento)\n",
        "macro_f1 = f1_score(Y_classe_treinamento, Y_previsoes2, average='macro')\n",
        "micro_f1 = f1_score(Y_classe_treinamento, Y_previsoes2, average='micro')\n",
        "balanced_acc = balanced_accuracy_score(Y_classe_treinamento, Y_previsoes2)\n",
        "mcc = matthews_corrcoef(Y_classe_treinamento, Y_previsoes2)\n",
        "\n",
        "# Imprimindo as métricas\n",
        "print(\"Macro F1 Score:\", macro_f1)\n",
        "print(\"Micro F1 Score:\", micro_f1)\n",
        "print(\"Balanced Accuracy:\", balanced_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaIUhZFljWol"
      },
      "outputs": [],
      "source": [
        "for i in range(100):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW3s0yOdBQNN"
      },
      "source": [
        "# **Decision Tree f1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4kH2n8r_ctD"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Definindo a grade de parâmetros para o GridSearchCV\n",
        "parametros = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'min_samples_split': [2, 3, 5],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'max_features': [None, 'sqrt', 'log2'],\n",
        "    'splitter': ['best', 'random']\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "dt_macro_f1_scores = []\n",
        "dt_micro_f1_scores = []\n",
        "dt_balanced_accuracies = []\n",
        "dt_mcc_scores = []\n",
        "dt_best_params_list = []\n",
        "dt_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(\n",
        "        X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Configurando o StratifiedKFold\n",
        "    stratified_k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=stratified_k_fold,\n",
        "                               n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    dt_melhores_parametros = grid_search.best_params_\n",
        "    dt_melhor_resultado = grid_search.best_score_\n",
        "    dt_best_params_list.append(dt_melhores_parametros)\n",
        "    dt_best_scores_list.append(dt_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_dt = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_dt.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    dt_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    dt_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    dt_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    dt_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    dt_macro_f1_scores.append(dt_macro_f1)\n",
        "    dt_micro_f1_scores.append(dt_micro_f1)\n",
        "    dt_balanced_accuracies.append(dt_balanced_acc)\n",
        "    dt_mcc_scores.append(dt_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "dt_mean_macro_f1 = np.mean(dt_macro_f1_scores)\n",
        "dt_std_macro_f1 = np.std(dt_macro_f1_scores)\n",
        "dt_mean_micro_f1 = np.mean(dt_micro_f1_scores)\n",
        "dt_std_micro_f1 = np.std(dt_micro_f1_scores)\n",
        "dt_mean_balanced_acc = np.mean(dt_balanced_accuracies)\n",
        "dt_std_balanced_acc = np.std(dt_balanced_accuracies)\n",
        "dt_mean_mcc = np.mean(dt_mcc_scores)\n",
        "dt_std_mcc = np.std(dt_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {dt_mean_macro_f1:.4f} (Desvio Padrão: {dt_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {dt_mean_micro_f1:.4f} (Desvio Padrão: {dt_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {dt_mean_balanced_acc:.4f} (Desvio Padrão: {dt_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {dt_mean_mcc:.4f} (Desvio Padrão: {dt_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "dt_best_overall_params = dt_best_params_list[np.argmax(dt_best_scores_list)]\n",
        "dt_best_overall_score = max(dt_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {dt_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {dt_best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLBH7Gan_zOt"
      },
      "source": [
        "## **Salvando resultados DT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnq_xdCN_uVG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blSA77MC_wIZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/dt_results.pkl'\n",
        "\n",
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'dt_macro_f1_scores': dt_macro_f1_scores,\n",
        "        'dt_micro_f1_scores': dt_micro_f1_scores,\n",
        "        'dt_balanced_accuracies': dt_balanced_accuracies,\n",
        "        'dt_mcc_scores': dt_mcc_scores,\n",
        "        'dt_best_params_list': dt_best_params_list,\n",
        "        'dt_best_scores_list': dt_best_scores_list,\n",
        "        'dt_mean_macro_f1': dt_mean_macro_f1,\n",
        "        'dt_std_macro_f1': dt_std_macro_f1,\n",
        "        'dt_mean_micro_f1': dt_mean_micro_f1,\n",
        "        'dt_std_micro_f1': dt_std_micro_f1,\n",
        "        'dt_mean_balanced_acc': dt_mean_balanced_acc,\n",
        "        'dt_std_balanced_acc': dt_std_balanced_acc,\n",
        "        'dt_mean_mcc': dt_mean_mcc,\n",
        "        'dt_std_mcc': dt_std_mcc,\n",
        "        'dt_best_overall_params': dt_best_overall_params,\n",
        "        'dt_best_overall_score': dt_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhF_KgpW_2ly"
      },
      "source": [
        "## **Carregando resultados DT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8qoNXQZ_xAC"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/dt_results.pkl'\n",
        "\n",
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        dt_macro_f1_scores = data['dt_macro_f1_scores']\n",
        "        dt_micro_f1_scores = data['dt_micro_f1_scores']\n",
        "        dt_balanced_accuracies = data['dt_balanced_accuracies']\n",
        "        dt_mcc_scores = data['dt_mcc_scores']\n",
        "        dt_best_params_list = data['dt_best_params_list']\n",
        "        dt_best_scores_list = data['dt_best_scores_list']\n",
        "        dt_mean_macro_f1 = data['dt_mean_macro_f1']\n",
        "        dt_std_macro_f1 = data['dt_std_macro_f1']\n",
        "        dt_mean_micro_f1 = data['dt_mean_micro_f1']\n",
        "        dt_std_micro_f1 = data['dt_std_micro_f1']\n",
        "        dt_mean_balanced_acc = data['dt_mean_balanced_acc']\n",
        "        dt_std_balanced_acc = data['dt_std_balanced_acc']\n",
        "        dt_mean_mcc = data['dt_mean_mcc']\n",
        "        dt_std_mcc = data['dt_std_mcc']\n",
        "        dt_best_overall_params = data['dt_best_overall_params']\n",
        "        dt_best_overall_score = data['dt_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    dt_macro_f1_scores = []\n",
        "    dt_micro_f1_scores = []\n",
        "    dt_balanced_accuracies = []\n",
        "    dt_mcc_scores = []\n",
        "    dt_best_params_list = []\n",
        "    dt_best_scores_list = []\n",
        "    dt_mean_macro_f1 = 0\n",
        "    dt_std_macro_f1 = 0\n",
        "    dt_mean_micro_f1 = 0\n",
        "    dt_std_micro_f1 = 0\n",
        "    dt_mean_balanced_acc = 0\n",
        "    dt_std_balanced_acc = 0\n",
        "    dt_mean_mcc = 0\n",
        "    dt_std_mcc = 0\n",
        "    dt_best_overall_params = None\n",
        "    dt_best_overall_score = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMB4TPvQe2Rs"
      },
      "source": [
        "# **Regressão logística%**\n",
        "\n",
        "*   Item da lista\n",
        "*   Item da lista\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zkYalWwEGyM"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Definindo a grade de parâmetros para o GridSearchCV\n",
        "parametros = {'tol': [0.0001, 0.00001, 0.000001],\n",
        "              'C': [0.01, 0.1, 1, 1.5, 2, 5, 10, 100],\n",
        "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "lr_macro_f1_scores = []\n",
        "lr_micro_f1_scores = []\n",
        "lr_balanced_accuracies = []\n",
        "lr_mcc_scores = []\n",
        "lr_best_params_list = []\n",
        "lr_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(\n",
        "        X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Configurando o StratifiedKFold\n",
        "    stratified_k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=LogisticRegression(random_state=0),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=stratified_k_fold,\n",
        "                               n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    lr_melhores_parametros = grid_search.best_params_\n",
        "    lr_melhor_resultado = grid_search.best_score_\n",
        "    lr_best_params_list.append(lr_melhores_parametros)\n",
        "    lr_best_scores_list.append(lr_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_lr = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_lr.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    lr_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    lr_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    lr_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    lr_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    lr_macro_f1_scores.append(lr_macro_f1)\n",
        "    lr_micro_f1_scores.append(lr_micro_f1)\n",
        "    lr_balanced_accuracies.append(lr_balanced_acc)\n",
        "    lr_mcc_scores.append(lr_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "lr_mean_macro_f1 = np.mean(lr_macro_f1_scores)\n",
        "lr_std_macro_f1 = np.std(lr_macro_f1_scores)\n",
        "lr_mean_micro_f1 = np.mean(lr_micro_f1_scores)\n",
        "lr_std_micro_f1 = np.std(lr_micro_f1_scores)\n",
        "lr_mean_balanced_acc = np.mean(lr_balanced_accuracies)\n",
        "lr_std_balanced_acc = np.std(lr_balanced_accuracies)\n",
        "lr_mean_mcc = np.mean(lr_mcc_scores)\n",
        "lr_std_mcc = np.std(lr_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {lr_mean_macro_f1:.4f} (Desvio Padrão: {lr_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {lr_mean_micro_f1:.4f} (Desvio Padrão: {lr_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {lr_mean_balanced_acc:.4f} (Desvio Padrão: {lr_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {lr_mean_mcc:.4f} (Desvio Padrão: {lr_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "lr_best_overall_params = lr_best_params_list[np.argmax(lr_best_scores_list)]\n",
        "lr_best_overall_score = max(lr_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {lr_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {lr_best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br4xe46mQUBn"
      },
      "outputs": [],
      "source": [
        "lr_best_overall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRe9nu7TEVLf"
      },
      "source": [
        "## **Salvando resutados LR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "genm4sSzEUrB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfubv4CrEUlD"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/lr_results.pkl'\n",
        "\n",
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'lr_macro_f1_scores': lr_macro_f1_scores,\n",
        "        'lr_micro_f1_scores': lr_micro_f1_scores,\n",
        "        'lr_balanced_accuracies': lr_balanced_accuracies,\n",
        "        'lr_mcc_scores': lr_mcc_scores,\n",
        "        'lr_best_params_list': lr_best_params_list,\n",
        "        'lr_best_scores_list': lr_best_scores_list,\n",
        "        'lr_mean_macro_f1': lr_mean_macro_f1,\n",
        "        'lr_std_macro_f1': lr_std_macro_f1,\n",
        "        'lr_mean_micro_f1': lr_mean_micro_f1,\n",
        "        'lr_std_micro_f1': lr_std_micro_f1,\n",
        "        'lr_mean_balanced_acc': lr_mean_balanced_acc,\n",
        "        'lr_std_balanced_acc': lr_std_balanced_acc,\n",
        "        'lr_mean_mcc': lr_mean_mcc,\n",
        "        'lr_std_mcc': lr_std_mcc,\n",
        "        'lr_best_overall_params': lr_best_overall_params,\n",
        "        'lr_best_overall_score': lr_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3yxemDFEZHS"
      },
      "source": [
        "## **Carregando resultados LR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lfVhy17EUVn"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/lr_results.pkl'\n",
        "\n",
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        lr_macro_f1_scores = data['lr_macro_f1_scores']\n",
        "        lr_micro_f1_scores = data['lr_micro_f1_scores']\n",
        "        lr_balanced_accuracies = data['lr_balanced_accuracies']\n",
        "        lr_mcc_scores = data['lr_mcc_scores']\n",
        "        lr_best_params_list = data['lr_best_params_list']\n",
        "        lr_best_scores_list = data['lr_best_scores_list']\n",
        "        lr_mean_macro_f1 = data['lr_mean_macro_f1']\n",
        "        lr_std_macro_f1 = data['lr_std_macro_f1']\n",
        "        lr_mean_micro_f1 = data['lr_mean_micro_f1']\n",
        "        lr_std_micro_f1 = data['lr_std_micro_f1']\n",
        "        lr_mean_balanced_acc = data['lr_mean_balanced_acc']\n",
        "        lr_std_balanced_acc = data['lr_std_balanced_acc']\n",
        "        lr_mean_mcc = data['lr_mean_mcc']\n",
        "        lr_std_mcc = data['lr_std_mcc']\n",
        "        lr_best_overall_params = data['lr_best_overall_params']\n",
        "        lr_best_overall_score = data['lr_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    lr_macro_f1_scores = []\n",
        "    lr_micro_f1_scores = []\n",
        "    lr_balanced_accuracies = []\n",
        "    lr_mcc_scores = []\n",
        "    lr_best_params_list = []\n",
        "    lr_best_scores_list = []\n",
        "    lr_mean_macro_f1 = 0\n",
        "    lr_std_macro_f1 = 0\n",
        "    lr_mean_micro_f1 = 0\n",
        "    lr_std_micro_f1 = 0\n",
        "    lr_mean_balanced_acc = 0\n",
        "    lr_std_balanced_acc = 0\n",
        "    lr_mean_mcc = 0\n",
        "    lr_std_mcc = 0\n",
        "    lr_best_overall_params = None\n",
        "    lr_best_overall_score = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNEKz1RL9xNT"
      },
      "source": [
        "# **Redes Neurais**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekeJePXLAhGS"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Definindo os parâmetros para a grid search\n",
        "parametros = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001, 0.001],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [5000],\n",
        "    'learning_rate_init': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "mlp_macro_f1_scores = []\n",
        "mlp_micro_f1_scores = []\n",
        "mlp_balanced_accuracies = []\n",
        "mlp_mcc_scores = []\n",
        "mlp_best_params_list = []\n",
        "mlp_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(\n",
        "        X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Configurando o StratifiedKFold\n",
        "    stratified_k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=MLPClassifier(random_state=0, tol=0.0001),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=stratified_k_fold,\n",
        "                               n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    mlp_melhores_parametros = grid_search.best_params_\n",
        "    mlp_melhor_resultado = grid_search.best_score_\n",
        "    mlp_best_params_list.append(mlp_melhores_parametros)\n",
        "    mlp_best_scores_list.append(mlp_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_mlp = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_mlp.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    mlp_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    mlp_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    mlp_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    mlp_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    mlp_macro_f1_scores.append(mlp_macro_f1)\n",
        "    mlp_micro_f1_scores.append(mlp_micro_f1)\n",
        "    mlp_balanced_accuracies.append(mlp_balanced_acc)\n",
        "    mlp_mcc_scores.append(mlp_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "mlp_mean_macro_f1 = np.mean(mlp_macro_f1_scores)\n",
        "mlp_std_macro_f1 = np.std(mlp_macro_f1_scores)\n",
        "mlp_mean_micro_f1 = np.mean(mlp_micro_f1_scores)\n",
        "mlp_std_micro_f1 = np.std(mlp_micro_f1_scores)\n",
        "mlp_mean_balanced_acc = np.mean(mlp_balanced_accuracies)\n",
        "mlp_std_balanced_acc = np.std(mlp_balanced_accuracies)\n",
        "mlp_mean_mcc = np.mean(mlp_mcc_scores)\n",
        "mlp_std_mcc = np.std(mlp_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {mlp_mean_macro_f1:.4f} (Desvio Padrão: {mlp_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {mlp_mean_micro_f1:.4f} (Desvio Padrão: {mlp_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {mlp_mean_balanced_acc:.4f} (Desvio Padrão: {mlp_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {mlp_mean_mcc:.4f} (Desvio Padrão: {mlp_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "mlp_best_overall_params = mlp_best_params_list[np.argmax(mlp_best_scores_list)]\n",
        "mlp_best_overall_score = max(mlp_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {mlp_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {mlp_best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F81QSRqaAsJh"
      },
      "source": [
        "## **Salvando resultados MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2PiWGIwAmQf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GEJrL13AmCF"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/mlp_results.pkl'\n",
        "\n",
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'mlp_macro_f1_scores': mlp_macro_f1_scores,\n",
        "        'mlp_micro_f1_scores': mlp_micro_f1_scores,\n",
        "        'mlp_balanced_accuracies': mlp_balanced_accuracies,\n",
        "        'mlp_mcc_scores': mlp_mcc_scores,\n",
        "        'mlp_best_params_list': mlp_best_params_list,\n",
        "        'mlp_best_scores_list': mlp_best_scores_list,\n",
        "        'mlp_mean_macro_f1': mlp_mean_macro_f1,\n",
        "        'mlp_std_macro_f1': mlp_std_macro_f1,\n",
        "        'mlp_mean_micro_f1': mlp_mean_micro_f1,\n",
        "        'mlp_std_micro_f1': mlp_std_micro_f1,\n",
        "        'mlp_mean_balanced_acc': mlp_mean_balanced_acc,\n",
        "        'mlp_std_balanced_acc': mlp_std_balanced_acc,\n",
        "        'mlp_mean_mcc': mlp_mean_mcc,\n",
        "        'mlp_std_mcc': mlp_std_mcc,\n",
        "        'mlp_best_overall_params': mlp_best_overall_params,\n",
        "        'mlp_best_overall_score': mlp_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89CW-nUhAwla"
      },
      "source": [
        "## **Carregando resultados MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAHWWpcOApSA"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/mlp_results.pkl'\n",
        "\n",
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        mlp_macro_f1_scores = data['mlp_macro_f1_scores']\n",
        "        mlp_micro_f1_scores = data['mlp_micro_f1_scores']\n",
        "        mlp_balanced_accuracies = data['mlp_balanced_accuracies']\n",
        "        mlp_mcc_scores = data['mlp_mcc_scores']\n",
        "        mlp_best_params_list = data['mlp_best_params_list']\n",
        "        mlp_best_scores_list = data['mlp_best_scores_list']\n",
        "        mlp_mean_macro_f1 = data['mlp_mean_macro_f1']\n",
        "        mlp_std_macro_f1 = data['mlp_std_macro_f1']\n",
        "        mlp_mean_micro_f1 = data['mlp_mean_micro_f1']\n",
        "        mlp_std_micro_f1 = data['mlp_std_micro_f1']\n",
        "        mlp_mean_balanced_acc = data['mlp_mean_balanced_acc']\n",
        "        mlp_std_balanced_acc = data['mlp_std_balanced_acc']\n",
        "        mlp_mean_mcc = data['mlp_mean_mcc']\n",
        "        mlp_std_mcc = data['mlp_std_mcc']\n",
        "        mlp_best_overall_params = data['mlp_best_overall_params']\n",
        "        mlp_best_overall_score = data['mlp_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    mlp_macro_f1_scores = []\n",
        "    mlp_micro_f1_scores = []\n",
        "    mlp_balanced_accuracies = []\n",
        "    mlp_mcc_scores = []\n",
        "    mlp_best_params_list = []\n",
        "    mlp_best_scores_list = []\n",
        "    mlp_mean_macro_f1 = 0\n",
        "    mlp_std_macro_f1 = 0\n",
        "    mlp_mean_micro_f1 = 0\n",
        "    mlp_std_micro_f1 = 0\n",
        "    mlp_mean_balanced_acc = 0\n",
        "    mlp_std_balanced_acc = 0\n",
        "    mlp_mean_mcc = 0\n",
        "    mlp_std_mcc = 0\n",
        "    mlp_best_overall_params = None\n",
        "    mlp_best_overall_score = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeCVxbE6yr9Y"
      },
      "source": [
        "# **SVM **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4uywyfwGIeR"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Definindo a grade de parâmetros para o GridSearchCV\n",
        "parametros = {\n",
        "    'tol': [0.01, 0.001, 0.0005, 0.0001, 0.00005, 0.00001],\n",
        "    'C': [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 5.0],\n",
        "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "    'degree': [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "svc_macro_f1_scores = []\n",
        "svc_micro_f1_scores = []\n",
        "svc_balanced_accuracies = []\n",
        "svc_mcc_scores = []\n",
        "svc_best_params_list = []\n",
        "svc_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(\n",
        "        X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Configurando o StratifiedKFold\n",
        "    stratified_k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=SVC(probability=True, random_state=0),\n",
        "                               param_grid=parametros, cv=stratified_k_fold, scoring=macro_f1_scorer, n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    svc_melhores_parametros = grid_search.best_params_\n",
        "    svc_melhor_resultado = grid_search.best_score_\n",
        "    svc_best_params_list.append(svc_melhores_parametros)\n",
        "    svc_best_scores_list.append(svc_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_svc = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_svc.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    svc_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    svc_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    svc_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    svc_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    svc_macro_f1_scores.append(svc_macro_f1)\n",
        "    svc_micro_f1_scores.append(svc_micro_f1)\n",
        "    svc_balanced_accuracies.append(svc_balanced_acc)\n",
        "    svc_mcc_scores.append(svc_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "svc_mean_macro_f1 = np.mean(svc_macro_f1_scores)\n",
        "svc_std_macro_f1 = np.std(svc_macro_f1_scores)\n",
        "svc_mean_micro_f1 = np.mean(svc_micro_f1_scores)\n",
        "svc_std_micro_f1 = np.std(svc_micro_f1_scores)\n",
        "svc_mean_balanced_acc = np.mean(svc_balanced_accuracies)\n",
        "svc_std_balanced_acc = np.std(svc_balanced_accuracies)\n",
        "svc_mean_mcc = np.mean(svc_mcc_scores)\n",
        "svc_std_mcc = np.std(svc_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {svc_mean_macro_f1:.4f} (Desvio Padrão: {svc_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {svc_mean_micro_f1:.4f} (Desvio Padrão: {svc_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {svc_mean_balanced_acc:.4f} (Desvio Padrão: {svc_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {svc_mean_mcc:.4f} (Desvio Padrão: {svc_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "svc_best_overall_params = svc_best_params_list[np.argmax(svc_best_scores_list)]\n",
        "svc_best_overall_score = max(svc_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {svc_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {svc_best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxf66ggdGLIg"
      },
      "source": [
        "## **Salvando resultados SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faD33j2hGKzn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zF0bFbwGKr9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/svc_results.pkl'\n",
        "\n",
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'svc_macro_f1_scores': svc_macro_f1_scores,\n",
        "        'svc_micro_f1_scores': svc_micro_f1_scores,\n",
        "        'svc_balanced_accuracies': svc_balanced_accuracies,\n",
        "        'svc_mcc_scores': svc_mcc_scores,\n",
        "        'svc_best_params_list': svc_best_params_list,\n",
        "        'svc_best_scores_list': svc_best_scores_list,\n",
        "        'svc_mean_macro_f1': svc_mean_macro_f1,\n",
        "        'svc_std_macro_f1': svc_std_macro_f1,\n",
        "        'svc_mean_micro_f1': svc_mean_micro_f1,\n",
        "        'svc_std_micro_f1': svc_std_micro_f1,\n",
        "        'svc_mean_balanced_acc': svc_mean_balanced_acc,\n",
        "        'svc_std_balanced_acc': svc_std_balanced_acc,\n",
        "        'svc_mean_mcc': svc_mean_mcc,\n",
        "        'svc_std_mcc': svc_std_mcc,\n",
        "        'svc_best_overall_params': svc_best_overall_params,\n",
        "        'svc_best_overall_score': svc_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BUhUoZ9Hn6L"
      },
      "source": [
        "## **Carregando resultados SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJcjnk-QGKnc"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/svc_results.pkl'\n",
        "\n",
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        svc_macro_f1_scores = data['svc_macro_f1_scores']\n",
        "        svc_micro_f1_scores = data['svc_micro_f1_scores']\n",
        "        svc_balanced_accuracies = data['svc_balanced_accuracies']\n",
        "        svc_mcc_scores = data['svc_mcc_scores']\n",
        "        svc_best_params_list = data['svc_best_params_list']\n",
        "        svc_best_scores_list = data['svc_best_scores_list']\n",
        "        svc_mean_macro_f1 = data['svc_mean_macro_f1']\n",
        "        svc_std_macro_f1 = data['svc_std_macro_f1']\n",
        "        svc_mean_micro_f1 = data['svc_mean_micro_f1']\n",
        "        svc_std_micro_f1 = data['svc_std_micro_f1']\n",
        "        svc_mean_balanced_acc = data['svc_mean_balanced_acc']\n",
        "        svc_std_balanced_acc = data['svc_std_balanced_acc']\n",
        "        svc_mean_mcc = data['svc_mean_mcc']\n",
        "        svc_std_mcc = data['svc_std_mcc']\n",
        "        svc_best_overall_params = data['svc_best_overall_params']\n",
        "        svc_best_overall_score = data['svc_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    svc_macro_f1_scores = []\n",
        "    svc_micro_f1_scores = []\n",
        "    svc_balanced_accuracies = []\n",
        "    svc_mcc_scores = []\n",
        "    svc_best_params_list = []\n",
        "    svc_best_scores_list = []\n",
        "    svc_mean_macro_f1 = 0\n",
        "    svc_std_macro_f1 = 0\n",
        "    svc_mean_micro_f1 = 0\n",
        "    svc_std_micro_f1 = 0\n",
        "    svc_mean_balanced_acc = 0\n",
        "    svc_std_balanced_acc = 0\n",
        "    svc_mean_mcc = 0\n",
        "    svc_std_mcc = 0\n",
        "    svc_best_overall_params = None\n",
        "    svc_best_overall_score = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW-LR5yyAdIX"
      },
      "source": [
        "# **Random Forest%**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eAe0gN5PIURu"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o parâmetro de avaliação\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Definindo a grade de parâmetros para o GridSearchCV\n",
        "parametros = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Listas para armazenar as métricas\n",
        "rf_macro_f1_scores = []\n",
        "rf_micro_f1_scores = []\n",
        "rf_balanced_accuracies = []\n",
        "rf_mcc_scores = []\n",
        "rf_best_params_list = []\n",
        "rf_best_scores_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Dividindo os dados em treinamento e teste com uma semente diferente a cada iteração\n",
        "    X_criterios_treinamento, X_criterios_teste, Y_classe_treinamento, Y_classe_teste = train_test_split(\n",
        "        X_criterios, Y_classe, test_size=0.20, random_state=i)\n",
        "\n",
        "    # Configurando o StratifiedKFold\n",
        "    stratified_k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
        "\n",
        "    # Executando o GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=0),\n",
        "                               param_grid=parametros,\n",
        "                               scoring=scorer,\n",
        "                               cv=stratified_k_fold,\n",
        "                               n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_criterios_treinamento, Y_classe_treinamento)\n",
        "\n",
        "    # Armazenando os melhores parâmetros e melhor resultado\n",
        "    rf_melhores_parametros = grid_search.best_params_\n",
        "    rf_melhor_resultado = grid_search.best_score_\n",
        "    rf_best_params_list.append(rf_melhores_parametros)\n",
        "    rf_best_scores_list.append(rf_melhor_resultado)\n",
        "\n",
        "    # Melhor modelo encontrado pelo GridSearchCV\n",
        "    best_rf = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões nos dados de teste\n",
        "    Y_previsoes = best_rf.predict(X_criterios_teste)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    rf_macro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='macro')\n",
        "    rf_micro_f1 = f1_score(Y_classe_teste, Y_previsoes, average='micro')\n",
        "    rf_balanced_acc = balanced_accuracy_score(Y_classe_teste, Y_previsoes)\n",
        "    rf_mcc = matthews_corrcoef(Y_classe_teste, Y_previsoes)\n",
        "\n",
        "    # Armazenando as métricas\n",
        "    rf_macro_f1_scores.append(rf_macro_f1)\n",
        "    rf_micro_f1_scores.append(rf_micro_f1)\n",
        "    rf_balanced_accuracies.append(rf_balanced_acc)\n",
        "    rf_mcc_scores.append(rf_mcc)\n",
        "\n",
        "# Calculando médias e desvios padrão das métricas\n",
        "rf_mean_macro_f1 = np.mean(rf_macro_f1_scores)\n",
        "rf_std_macro_f1 = np.std(rf_macro_f1_scores)\n",
        "rf_mean_micro_f1 = np.mean(rf_micro_f1_scores)\n",
        "rf_std_micro_f1 = np.std(rf_micro_f1_scores)\n",
        "rf_mean_balanced_acc = np.mean(rf_balanced_accuracies)\n",
        "rf_std_balanced_acc = np.std(rf_balanced_accuracies)\n",
        "rf_mean_mcc = np.mean(rf_mcc_scores)\n",
        "rf_std_mcc = np.std(rf_mcc_scores)\n",
        "\n",
        "# Imprimindo as métricas calculadas\n",
        "print(f\"Média Macro F1 Score: {rf_mean_macro_f1:.4f} (Desvio Padrão: {rf_std_macro_f1:.4f})\")\n",
        "print(f\"Média Micro F1 Score: {rf_mean_micro_f1:.4f} (Desvio Padrão: {rf_std_micro_f1:.4f})\")\n",
        "print(f\"Média Balanced Accuracy: {rf_mean_balanced_acc:.4f} (Desvio Padrão: {rf_std_balanced_acc:.4f})\")\n",
        "print(f\"Média MCC: {rf_mean_mcc:.4f} (Desvio Padrão: {rf_std_mcc:.4f})\")\n",
        "\n",
        "# Identificando os melhores parâmetros gerais e o melhor score\n",
        "rf_best_overall_params = rf_best_params_list[np.argmax(rf_best_scores_list)]\n",
        "rf_best_overall_score = max(rf_best_scores_list)\n",
        "\n",
        "print(f\"Melhores parâmetros gerais: {rf_best_overall_params}\")\n",
        "print(f\"Melhor score geral: {rf_best_overall_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyXHbYqzMq1O"
      },
      "source": [
        "## **Salvando resultados RF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bDf1YZ8Mql5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIZb5b-vMqhf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/rf_results.pkl'\n",
        "\n",
        "# Salvando os valores das variáveis em um arquivo\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'rf_macro_f1_scores': rf_macro_f1_scores,\n",
        "        'rf_micro_f1_scores': rf_micro_f1_scores,\n",
        "        'rf_balanced_accuracies': rf_balanced_accuracies,\n",
        "        'rf_mcc_scores': rf_mcc_scores,\n",
        "        'rf_best_params_list': rf_best_params_list,\n",
        "        'rf_best_scores_list': rf_best_scores_list,\n",
        "        'rf_mean_macro_f1': rf_mean_macro_f1,\n",
        "        'rf_std_macro_f1': rf_std_macro_f1,\n",
        "        'rf_mean_micro_f1': rf_mean_micro_f1,\n",
        "        'rf_std_micro_f1': rf_std_micro_f1,\n",
        "        'rf_mean_balanced_acc': rf_mean_balanced_acc,\n",
        "        'rf_std_balanced_acc': rf_std_balanced_acc,\n",
        "        'rf_mean_mcc': rf_mean_mcc,\n",
        "        'rf_std_mcc': rf_std_mcc,\n",
        "        'rf_best_overall_params': rf_best_overall_params,\n",
        "        'rf_best_overall_score': rf_best_overall_score\n",
        "    }, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6EeWqleMx5D"
      },
      "source": [
        "## **Carregando resultados RF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LBLkt-oMqdS"
      },
      "outputs": [],
      "source": [
        "# Caminho no Google Drive\n",
        "file_path = '/content/drive/My Drive/rf_results.pkl'\n",
        "\n",
        "# Carregando os valores das variáveis a partir de um arquivo\n",
        "try:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "        rf_macro_f1_scores = data['rf_macro_f1_scores']\n",
        "        rf_micro_f1_scores = data['rf_micro_f1_scores']\n",
        "        rf_balanced_accuracies = data['rf_balanced_accuracies']\n",
        "        rf_mcc_scores = data['rf_mcc_scores']\n",
        "        rf_best_params_list = data['rf_best_params_list']\n",
        "        rf_best_scores_list = data['rf_best_scores_list']\n",
        "        rf_mean_macro_f1 = data['rf_mean_macro_f1']\n",
        "        rf_std_macro_f1 = data['rf_std_macro_f1']\n",
        "        rf_mean_micro_f1 = data['rf_mean_micro_f1']\n",
        "        rf_std_micro_f1 = data['rf_std_micro_f1']\n",
        "        rf_mean_balanced_acc = data['rf_mean_balanced_acc']\n",
        "        rf_std_balanced_acc = data['rf_std_balanced_acc']\n",
        "        rf_mean_mcc = data['rf_mean_mcc']\n",
        "        rf_std_mcc = data['rf_std_mcc']\n",
        "        rf_best_overall_params = data['rf_best_overall_params']\n",
        "        rf_best_overall_score = data['rf_best_overall_score']\n",
        "except FileNotFoundError:\n",
        "    rf_macro_f1_scores = []\n",
        "    rf_micro_f1_scores = []\n",
        "    rf_balanced_accuracies = []\n",
        "    rf_mcc_scores = []\n",
        "    rf_best_params_list = []\n",
        "    rf_best_scores_list = []\n",
        "    rf_mean_macro_f1 = 0\n",
        "    rf_std_macro_f1 = 0\n",
        "    rf_mean_micro_f1 = 0\n",
        "    rf_std_micro_f1 = 0\n",
        "    rf_mean_balanced_acc = 0\n",
        "    rf_std_balanced_acc = 0\n",
        "    rf_mean_mcc = 0\n",
        "    rf_std_mcc = 0\n",
        "    rf_best_overall_params = None\n",
        "    rf_best_overall_score = 0\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TW3s0yOdBQNN",
        "PMB4TPvQe2Rs",
        "WeCVxbE6yr9Y",
        "LW-LR5yyAdIX"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}